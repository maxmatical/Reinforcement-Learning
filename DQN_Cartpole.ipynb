{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN - Cartpole.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxmatical/Reinforcement-Learning/blob/master/DQN_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3n0OeNH1P0",
        "colab_type": "text"
      },
      "source": [
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zqgJDg-GJHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "06e8ccbf-886a-4f5d-daaf-fce90ebb61ef"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.3)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.5)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.1.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_0jk-XGNPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d5a0f42d-367c-48c3-85c4-b4ce5d536104"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "# plt.imshow(env.render('rgb_array'))\n",
        "print(\"Observation space:\", env.observation_space.shape)\n",
        "print(\"Action space:\", env.action_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: (4,)\n",
            "Action space: Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYBusu7hGVRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBT7yEeiHvH2",
        "colab_type": "text"
      },
      "source": [
        "#Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5dII41GhRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def push(self, *args):\n",
        "        \"\"\"\n",
        "        \n",
        "        Adds a tuple of (s, a, s', r) to memory for experience replay \n",
        "        \n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None) # appends memory with 'None' to get [None, None, ...]\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position+1)%self.capacity\n",
        "\n",
        "    def sample(self, bs):\n",
        "        \n",
        "        \"\"\"samples from memory\"\"\"\n",
        "        \n",
        "        return random.sample(self.memory, bs)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlWBFLUTKdLj",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562HWnMQIvJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        \"\"\"\n",
        "        \n",
        "        input shape should be (n_channels x h x w)\n",
        "        \n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.input_shape = input_shape\n",
        "        \n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 5, stride = 2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 5, stride = 2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size = 5, stride = 2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.linear = nn.Linear(self.feature_size(), n_actions)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        out = F.relu(self.bn1(self.conv1(input)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = F.relu(self.bn3(self.conv3(out)))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "    \n",
        "        \n",
        "    def feature_size(self):\n",
        "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kalBxqYL8hO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3ee8dfd6-58ec-455a-e104-18c5ba44be68"
      },
      "source": [
        "# Test to see if implemented correctly\n",
        "\n",
        "tmp = torch.randn((16, 3, 32, 32)).to(device)\n",
        "a = DQN((3, 32, 32), 2).to(device)\n",
        "print(a(tmp).shape)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDWWeUlTN_NG",
        "colab_type": "text"
      },
      "source": [
        "# Input Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf2SWNOoMFQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "b627bc88-a23b-4261-a946-715262c00157"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "def get_screen():\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    #print(screen.shape)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADECAYAAACP3tqSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEw50M7NMONDNzDLhQDczy4QD3cwsE/8PEA394X9D2S4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yy-oWfwOfdM",
        "colab_type": "text"
      },
      "source": [
        "# Setting up training\n",
        "\n",
        "- Initialize hyperparameters\n",
        "- Create models\n",
        "- Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-Onm5I7OCRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "84d74c1d-33d8-4e1d-cea4-26a050b223a5"
      },
      "source": [
        "# Setting hyperparameters for the model\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "init_screen = get_screen()\n",
        "\n",
        "_, n_channels, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "print('Input shape is', (n_channels, screen_height, screen_width))\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "print('number of outputs is', n_actions)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is (3, 40, 90)\n",
            "number of outputs is 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gFQltkJOilI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining our policy and target Q networks\n",
        "# Policy_net is updated every step\n",
        "# Target_net is updated every n steps\n",
        "\n",
        "policy_net = DQN((n_channels, screen_height, screen_width), n_actions).to(device)\n",
        "target_net = DQN((n_channels, screen_height, screen_width), n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr = 3e-4)\n",
        "\n",
        "memory = ReplayMemory(capacity=10000)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    \"\"\"\n",
        "    \n",
        "    selects an action based on e-greedy\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    global steps_done\n",
        "    sample = random.random() #samples from uniform[0,1]\n",
        "    current_eps = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY) #decay epsilon\n",
        "    steps_done += 1\n",
        "    if sample > current_eps:\n",
        "        with torch.no_grad():\n",
        "        # t.max(1) will return largest column value of each row.\n",
        "        # second column on max result is index of where max element was found,\n",
        "        # so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    \n",
        "    else: \n",
        "        return torch.tensor([[random.randrange(n_actions)]], \\\n",
        "                            device=device, dtype=torch.long) # returns random action\n",
        "    \n",
        "episode_durations = []\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXmIAWEmR4GK",
        "colab_type": "text"
      },
      "source": [
        "# Training Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VGt012CPfrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return \n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    # Compute a mask of non-terminal states and concatenate the batch elements\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device = device, dtype=torch.uint8)\n",
        "    \n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None]) # list of non-final next states\n",
        "\n",
        "    state_batch = torch.cat(batch.state) # list of states in the batch\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    \n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    \n",
        "    # Computes the Q values of the state-action pair\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "    \n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    \n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device = device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach() # predicted using target_net\n",
        "    \n",
        "    # compute the expected Q value \n",
        "    expected_Q_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Compute hubert loss (better than L1/L2 loss)\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_Q_values.unsqueeze(1))\n",
        "    \n",
        "    # update the weights of the model\n",
        "    optimizer.zero_grad() \n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzTSSeFpf0iw",
        "colab_type": "text"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "running_reward is the average duration of the last 100 episodes. The game is considered solved when the running_reward is >195\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uS1d-oTUCTD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b19aed1e-608c-4177-eb64-0bc21125301f"
      },
      "source": [
        "# n_episodes = 50\n",
        "# for i_episode in range(n_episodes):\n",
        "duration = 10\n",
        "n_episodes = 1\n",
        "running_reward = (duration * 0.99) + (n_episodes * 0.01)\n",
        "\n",
        "while running_reward <195:\n",
        "    n_episodes +=1\n",
        "    env.reset() # initialize and reset the environment\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "\n",
        "    for t in count():\n",
        "        # perform an action and get a reward\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device = device)\n",
        "\n",
        "        # observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # store transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # optimize the policy_network\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "#             plot_durations()\n",
        "           \n",
        "            break\n",
        "        \n",
        "        \n",
        "    duration = episode_durations[-1]\n",
        "    running_reward = (duration * 0.99) + (n_episodes * 0.01)\n",
        "    \n",
        "    \n",
        "    # update the target network by copyin the policy network\n",
        "    if n_episodes% TARGET_UPDATE == 0 :\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "        \n",
        "    if n_episodes%500 ==0:\n",
        "        print(running_reward)\n",
        "\n",
        "\n",
        "print(\"Finished\")\n",
        "print(running_reward)\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.9\n",
            "19.9\n",
            "23.91\n",
            "29.9\n",
            "35.89\n",
            "38.91\n",
            "44.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5wYEny1c7j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}